Any language model can be turned into a language model that only outputs strings satisfying a particular grammar simply by modifying how logits are decoded into tokens.

https://arxiv.org/abs/2305.13971

Given a language L with decidable prefix membership   (for any string a, we can decide if there is a string b such that ab is in L):

We can simply filter out all tokens that make the current string fail prefix membership at each deciding step.

More generally, for languages without decidable prefix membership:

Tree search algorithms such as best first search need to be used to decode a valid sequence of tokens.

## Applications to ml theorem proving

can skip pre training phase and go straight to unsupervised rl minif2f style training