Any language model can be turned into a language model that only outputs strings satisfying a particular grammar simply by modifying how logits are decoded into tokens.

https://arxiv.org/abs/2305.13971

Given a language L with decidable prefix membership   (for any string a, we can decide if there is a string b such that ab is in L):

We can simply filter out all tokens that make the current string fail prefix membership at each deciding step.

More generally, for languages without decidable prefix membership:

Tree search algorithms such as best first search need to be used to decode a valid sequence of tokens.

## Applications to ml theorem proving

can skip pre training phase and go straight to unsupervised rl minif2f style training

### specific use case: find and fix style theorem proving networks

a find and fix network takes in a lean 4 tactic state and produce a partially completed lean tactic. The network is the run recursively on each of the holes in the partial tactic. An example of this style of network is deepseekprover.

Instead of considering the entire lean language as a formal language/grammar we would only consider the formal language of lean tactic terms (where holes are expressed using `_?`)


### specific case PoT

https://arxiv.org/abs/2211.12588
grammar constraints could guarantee a valid program is output everytime and can be done efficiently since Python is has decidable prefix membership.

grammar constraints could also allow to skip the pretraining step and only train on (prompt, output number) where reward scarcity can be mitigated using neutral symbolic search approaches.

